{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Math Question Answer Verification Competition\n",
        "\n",
        "## Starter Code"
      ],
      "metadata": {
        "id": "70hrNJwhYMjR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Borrowed from [official Unsloth implementation](https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing#scrollTo=MKX_XKs_BNZR)"
      ],
      "metadata": {
        "id": "kp8dK32_gOZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%capture\n",
        "# This cell will take time\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ],
      "metadata": {
        "id": "bA1lW9pzWwpk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34c7157c-1734-4118-cc3e-2bf60b8c7a54"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unsloth in /usr/local/lib/python3.10/dist-packages (2024.11.6)\n",
            "Found existing installation: unsloth 2024.11.6\n",
            "Uninstalling unsloth-2024.11.6:\n",
            "  Successfully uninstalled unsloth-2024.11.6\n",
            "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-kqwynj7x/unsloth_80d2fa2930b4479b92f3f67b9d62cb37\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-kqwynj7x/unsloth_80d2fa2930b4479b92f3f67b9d62cb37\n",
            "  Resolved https://github.com/unslothai/unsloth.git to commit f26d4e739ed507de7a9088da53d10fd02f58d160\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: unsloth-zoo>=2024.11.1 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.11.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.2)\n",
            "Requirement already satisfied: tyro in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.8.14)\n",
            "Requirement already satisfied: transformers>=4.46.1 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.46.2)\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.0)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.44.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.26.4)\n",
            "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.26.2)\n",
            "Requirement already satisfied: hf-transfer in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.8)\n",
            "Requirement already satisfied: bitsandbytes>=0.43.3 in /usr/local/lib/python3.10/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.44.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.10.10)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.12.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.20.3)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (from unsloth-zoo>=2024.11.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.0)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.10/dist-packages (from unsloth-zoo>=2024.11.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.1.1)\n",
            "Requirement already satisfied: trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in /usr/local/lib/python3.10/dist-packages (from unsloth-zoo>=2024.11.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.12.0)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from unsloth-zoo>=2024.11.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.13.2)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (13.9.4)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.7.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.2)\n",
            "Building wheels for collected packages: unsloth\n",
            "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unsloth: filename=unsloth-2024.11.7-py3-none-any.whl size=163138 sha256=1c07ec3ba43bc9d4cf32ff9fbf1304bcd0f46a6a84b88508fdc1a634db9c1da4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-x21_8sly/wheels/ed/d4/e9/76fb290ee3df0a5fc21ce5c2c788e29e9607a2353d8342fd0d\n",
            "Successfully built unsloth\n",
            "Installing collected packages: unsloth\n",
            "Successfully installed unsloth-2024.11.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n"
      ],
      "metadata": {
        "id": "zlpjJOhtW7g3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ],
      "metadata": {
        "id": "5GxOyBTkXJIG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2725d849-624a-4ab2-8ed4-faacb5bda03f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2024.11.6: Fast Llama patching. Transformers = 4.46.2.\n",
            "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.5.1+cu124. CUDA = 8.0. CUDA Toolkit = 12.4.\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model and wrap with LoRA adapters"
      ],
      "metadata": {
        "id": "jVgabGjM8G1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "id": "xy0iN0RJXMAX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Competition dataset"
      ],
      "metadata": {
        "id": "uNruHjDieGSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download and load competition dataset\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\")\n",
        "# print and see dataset\n",
        "dataset"
      ],
      "metadata": {
        "id": "3OMXJz4Z8jhJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd6f6db4-3a16-4b73-875c-e6b195b90f5f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['question', 'is_correct', 'answer', 'solution'],\n",
              "        num_rows: 1000000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['question', 'is_correct', 'answer', 'solution'],\n",
              "        num_rows: 10000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"You are a great mathematician and you are tasked with finding if an answer to a given maths question is correct or not. Yout response should be 'True' if correct, otherwise 'False'. Below is Question and Answer.\n",
        "\n",
        "\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Answer:\n",
        "{}\n",
        "\n",
        "### Explainaition\n",
        "\n",
        "### Output:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    question = examples[\"question\"]\n",
        "    ans       = examples[\"answer\"]\n",
        "    output      = examples[\"is_correct\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(question, ans, output):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DBpDwJA-bJ9K"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the training dataset and generate prompt for each datapoint\n",
        "\n",
        "train_dataset = dataset['train'].map(formatting_prompts_func, batched = True,)"
      ],
      "metadata": {
        "id": "fEeHyA68-puB"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print a smaple training example\n",
        "train_dataset['text'][0]"
      ],
      "metadata": {
        "id": "JKBG7s8woNuo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "e2c96cc2-f9a8-4af1-853e-3cb990f76793"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"You are a great mathematician and you are tasked with finding if an answer to a given maths question is correct or not. Yout response should be 'True' if correct, otherwise 'False'. Below is Question and Answer.\\n\\n\\n\\n### Question:\\nWhat is the radius of the circle inscribed in triangle $ABC$ if $AB = 22, AC=12,$ and $BC=14$? Express your answer in simplest radical form.\\n\\n### Answer:\\n3.16227766016838\\n\\n### Explainaition\\n\\n### Output:\\nTrue<|end_of_text|>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
        "dataset_split = dataset['train'].train_test_split(test_size=0.01)\n",
        "train_dataset = dataset_split['train']\n",
        "eval_dataset = dataset_split['test']\n",
        "\n",
        "# é‡æ–°ç”Ÿæˆ 'text' å­—æ®µ\n",
        "train_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n",
        "eval_dataset = eval_dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# æ£€æŸ¥ç”Ÿæˆçš„å­—æ®µæ˜¯å¦æ­£ç¡®\n",
        "print(\"train text:\", train_dataset.column_names)\n",
        "print(\"validate text:\", eval_dataset.column_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c38hKBZ1-vc5",
        "outputId": "079f0246-0d48-490d-eabc-315ce68a605f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train text: ['question', 'is_correct', 'answer', 'solution', 'text']\n",
            "validate text: ['question', 'is_correct', 'answer', 'solution', 'text']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SFT"
      ],
      "metadata": {
        "id": "egSQOrCJeM7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "metadata": {
        "id": "WsIzOw4JsReP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "\n",
        "# ä½¿ç”¨éªŒè¯é›†çš„å­é›†\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size = 4,\n",
        "    gradient_accumulation_steps = 4,  # å¢åŠ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œæå‡ç­‰æ•ˆæ‰¹æ¬¡\n",
        "    warmup_steps=100,                # å¢åŠ  warmup æ­¥æ•°ï¼Œé€‚åº”æ›´é•¿çš„è®­ç»ƒ\n",
        "    max_steps=1500,                  # å¢åŠ æ€»è®­ç»ƒæ­¥æ•°\n",
        "    learning_rate=2e-4,\n",
        "    # lr_scheduler_type=\"cosine\",  # ä½¿ç”¨cosineå­¦ä¹ ç‡è°ƒåº¦å™¨\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=10,\n",
        "    eval_steps=200,\n",
        "    evaluation_strategy=\"steps\",  # æ·»åŠ æ­¤è¡Œ\n",
        "    logging_strategy=\"steps\",     # æ·»åŠ æ­¤è¡Œ\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.02,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    seed=3407,\n",
        "    output_dir=\"outputs\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,  # æ·»åŠ éªŒè¯é›†ï¼Œç›‘æ§æ¨¡å‹æ•ˆæœ\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=4,\n",
        "    packing=False,\n",
        "    args=training_args\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82AeOjKMBvC9",
        "outputId": "473d4a8f-ba51-4204-9704-af6ff0922c7e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "GLsq_UFMBxqm",
        "outputId": "0bb3443e-8d5e-412f-d3b2-13ca877f52cc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 990,000 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 16 | Total steps = 1,500\n",
            " \"-____-\"     Number of trainable parameters = 41,943,040\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 1:34:07, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.670100</td>\n",
              "      <td>0.711822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.665300</td>\n",
              "      <td>0.677266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.626200</td>\n",
              "      <td>0.642554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.594300</td>\n",
              "      <td>0.606936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.566200</td>\n",
              "      <td>0.575659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.553800</td>\n",
              "      <td>0.553260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.503800</td>\n",
              "      <td>0.544168</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# å¯ç”¨å¿«é€Ÿæ¨ç†æ¨¡å¼\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# 1. éšæœºæŠ½å–1,000æ¡æ ·æœ¬\n",
        "sample_size = 1000\n",
        "eval_sample = eval_dataset.select(random.sample(range(len(eval_dataset)), sample_size))\n",
        "\n",
        "# 2. è®¾ç½®æ‰¹é‡å¤§å°\n",
        "batch_size = 32\n",
        "predictions = []\n",
        "true_labels = eval_sample[\"is_correct\"]\n",
        "\n",
        "# 3. æ‰¹é‡å¤„ç†æŠ½å–çš„æ ·æœ¬\n",
        "for i in range(0, len(eval_sample), batch_size):\n",
        "    batch = eval_sample.select(range(i, min(i + batch_size, len(eval_sample))))\n",
        "    input_prompts = [\n",
        "        f\"### Question:\\n{item['question']}\\n\\n### Answer:\\n{item['answer']}\\n\\n### Output:\\n\"\n",
        "        for item in batch\n",
        "    ]\n",
        "\n",
        "    # ä½¿ç”¨ tokenizer å¤„ç†æ‰¹æ¬¡è¾“å…¥\n",
        "    inputs = tokenizer(input_prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    # å°†è¾“å…¥æ•°æ®ç§»åŠ¨åˆ° GPUï¼ˆå¦‚æœæœ‰ï¼‰\n",
        "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()} if torch.cuda.is_available() else inputs\n",
        "\n",
        "    # æ‰¹é‡ç”Ÿæˆé¢„æµ‹ç»“æœ\n",
        "    outputs = model.generate(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=64)\n",
        "    responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    # å°†æ¯ä¸ªå“åº”è½¬åŒ–ä¸º True æˆ– False\n",
        "    batch_predictions = [response.strip() == \"True\" for response in responses]\n",
        "    predictions.extend(batch_predictions)\n",
        "\n",
        "# 4. è®¡ç®—æŠ½æ ·éªŒè¯é›†çš„å‡†ç¡®ç‡\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "print(f\"æŠ½æ ·éªŒè¯é›†å‡†ç¡®ç‡: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DD0sZ6fHJZLs",
        "outputId": "2c932447-1c39-4db2-91ab-db898ef4f575"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "æŠ½æ ·éªŒè¯é›†å‡†ç¡®ç‡: 0.60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# ç¡®ä¿å·²åŠ è½½æµ‹è¯•é›†\n",
        "test_dataset = dataset['test']\n",
        "\n",
        "# å¯ç”¨å¿«é€Ÿæ¨ç†æ¨¡å¼\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# åˆå§‹åŒ–é¢„æµ‹ç»“æœåˆ—è¡¨\n",
        "test_predictions = []\n",
        "\n",
        "# è®¾ç½®æ‰¹é‡å¤§å°\n",
        "batch_size = 32\n",
        "\n",
        "# ä½¿ç”¨ tqdm åŒ…è£…å¾ªç¯ä»¥æ˜¾ç¤ºè¿›åº¦æ¡\n",
        "for i in tqdm(range(0, len(test_dataset), batch_size), desc=\"Processing test dataset\"):\n",
        "    # åˆ†æ‰¹è·å–æµ‹è¯•æ•°æ®\n",
        "    batch = test_dataset.select(range(i, min(i + batch_size, len(test_dataset))))\n",
        "    input_prompts = [\n",
        "        f\"### Question:\\n{item['question']}\\n\\n### Answer:\\n{item['answer']}\\n\\n### Output:\\n\"\n",
        "        for item in batch\n",
        "    ]\n",
        "\n",
        "    # ä½¿ç”¨ tokenizer å¯¹æ‰¹æ¬¡è¿›è¡Œå¤„ç†\n",
        "    inputs = tokenizer(input_prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    # å°†è¾“å…¥æ•°æ®ç§»åŠ¨åˆ° GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
        "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()} if torch.cuda.is_available() else inputs\n",
        "\n",
        "    # æ‰¹é‡ç”Ÿæˆé¢„æµ‹ç»“æœ\n",
        "    outputs = model.generate(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=64)\n",
        "    responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    # è§£ææ¯ä¸ªå“åº”å¹¶å­˜å‚¨åˆ°åˆ—è¡¨\n",
        "    for j, response in enumerate(responses):\n",
        "        is_correct = response.strip() == \"True\"  # æ£€æŸ¥é¢„æµ‹ç»“æœæ˜¯å¦ä¸ºâ€œTrueâ€\n",
        "        test_predictions.append({\"ID\": i + j, \"is_correct\": is_correct})\n",
        "\n",
        "# å°†é¢„æµ‹ç»“æœä¿å­˜ä¸ºç¬¦åˆç«èµ›æ ¼å¼çš„CSVæ–‡ä»¶\n",
        "submission_df = pd.DataFrame(test_predictions)\n",
        "submission_df.to_csv(\"submission_competition.csv\", index=False)\n",
        "\n",
        "print(\"æµ‹è¯•é›†é¢„æµ‹ç»“æœå·²ç”Ÿæˆå¹¶ä¿å­˜ä¸º submission_competition.csv æ–‡ä»¶ã€‚\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ji05o2psP99o",
        "outputId": "1df268ab-7195-4aa0-bf55-aea4d58bfbb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [29:27<00:00,  5.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "æµ‹è¯•é›†é¢„æµ‹ç»“æœå·²ç”Ÿæˆå¹¶ä¿å­˜ä¸º submission_competition.csv æ–‡ä»¶ã€‚\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"submission_competition.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Ve4cdV3uXmW2",
        "outputId": "c023a8ff-7768-4f33-e80b-eb4d4403f133"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_45cfeb6f-0641-4722-8db8-79ff20f5b6ff\", \"submission_competition.csv\", 108904)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## inference"
      ],
      "metadata": {
        "id": "OjOqIXhCePfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from unsloth import FastLanguageModel\n",
        "test_dataset = dataset['test']\n",
        "# å¯ç”¨å¿«é€Ÿæ¨ç†æ¨¡å¼\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# éå†å‰20ä¸ªæ ·æœ¬å¹¶è¿›è¡Œæ¨ç†\n",
        "num_samples = 100\n",
        "results = []\n",
        "\n",
        "for i in tqdm(range(num_samples), desc=\"Running inference for 20 samples\"):\n",
        "    sample_ques = test_dataset['question'][i]\n",
        "    sample_ans = test_dataset['answer'][i]\n",
        "\n",
        "    # æ„é€ è¾“å…¥æç¤º\n",
        "    input_prompt = prompt.format(sample_ques, sample_ans, \"\")\n",
        "\n",
        "    # ç¼–ç è¾“å…¥æç¤º\n",
        "    inputs = tokenizer([input_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "    input_shape = inputs['input_ids'].shape\n",
        "    input_token_len = input_shape[1]\n",
        "\n",
        "    # ç”Ÿæˆå“åº”ï¼Œå¹¶é™åˆ¶ç”Ÿæˆé•¿åº¦\n",
        "    outputs = model.generate(**inputs, max_new_tokens=5, use_cache=True, eos_token_id=tokenizer.eos_token_id)\n",
        "    response = tokenizer.decode(outputs[0][input_token_len:], skip_special_tokens=True).strip()\n",
        "\n",
        "    # åˆ¤æ–­ç”Ÿæˆçš„å“åº”æ˜¯å¦ä¸ºâ€œTrueâ€æˆ–â€œFalseâ€\n",
        "    if 'true' in response.lower():\n",
        "        is_correct = 'True'\n",
        "    elif 'false' in response.lower():\n",
        "        is_correct = 'False'\n",
        "    else:\n",
        "        is_correct = \"missing\"  # è‹¥ç”Ÿæˆçš„ç»“æœæ— æ³•åˆ¤æ–­\n",
        "\n",
        "    # ä»…ä¿å­˜IDå’Œåˆ¤æ–­ç»“æœ\n",
        "    results.append({'ID': i, 'is_correct': is_correct})\n",
        "\n",
        "# æ‰“å°å‡ºæ‰€æœ‰ç»“æœ\n",
        "for result in results:\n",
        "    print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVeFxD7gdTic",
        "outputId": "5c23e5fa-b10b-442c-e5a3-22acceb43785"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running inference for 20 samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:39<00:00,  2.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ID': 0, 'is_correct': 'True'}\n",
            "{'ID': 1, 'is_correct': 'False'}\n",
            "{'ID': 2, 'is_correct': 'True'}\n",
            "{'ID': 3, 'is_correct': 'True'}\n",
            "{'ID': 4, 'is_correct': 'False'}\n",
            "{'ID': 5, 'is_correct': 'False'}\n",
            "{'ID': 6, 'is_correct': 'False'}\n",
            "{'ID': 7, 'is_correct': 'False'}\n",
            "{'ID': 8, 'is_correct': 'False'}\n",
            "{'ID': 9, 'is_correct': 'False'}\n",
            "{'ID': 10, 'is_correct': 'False'}\n",
            "{'ID': 11, 'is_correct': 'False'}\n",
            "{'ID': 12, 'is_correct': 'False'}\n",
            "{'ID': 13, 'is_correct': 'False'}\n",
            "{'ID': 14, 'is_correct': 'True'}\n",
            "{'ID': 15, 'is_correct': 'False'}\n",
            "{'ID': 16, 'is_correct': 'False'}\n",
            "{'ID': 17, 'is_correct': 'False'}\n",
            "{'ID': 18, 'is_correct': 'False'}\n",
            "{'ID': 19, 'is_correct': 'False'}\n",
            "{'ID': 20, 'is_correct': 'False'}\n",
            "{'ID': 21, 'is_correct': 'True'}\n",
            "{'ID': 22, 'is_correct': 'False'}\n",
            "{'ID': 23, 'is_correct': 'False'}\n",
            "{'ID': 24, 'is_correct': 'True'}\n",
            "{'ID': 25, 'is_correct': 'True'}\n",
            "{'ID': 26, 'is_correct': 'False'}\n",
            "{'ID': 27, 'is_correct': 'False'}\n",
            "{'ID': 28, 'is_correct': 'False'}\n",
            "{'ID': 29, 'is_correct': 'True'}\n",
            "{'ID': 30, 'is_correct': 'True'}\n",
            "{'ID': 31, 'is_correct': 'False'}\n",
            "{'ID': 32, 'is_correct': 'False'}\n",
            "{'ID': 33, 'is_correct': 'False'}\n",
            "{'ID': 34, 'is_correct': 'False'}\n",
            "{'ID': 35, 'is_correct': 'True'}\n",
            "{'ID': 36, 'is_correct': 'False'}\n",
            "{'ID': 37, 'is_correct': 'False'}\n",
            "{'ID': 38, 'is_correct': 'False'}\n",
            "{'ID': 39, 'is_correct': 'False'}\n",
            "{'ID': 40, 'is_correct': 'True'}\n",
            "{'ID': 41, 'is_correct': 'False'}\n",
            "{'ID': 42, 'is_correct': 'False'}\n",
            "{'ID': 43, 'is_correct': 'False'}\n",
            "{'ID': 44, 'is_correct': 'True'}\n",
            "{'ID': 45, 'is_correct': 'False'}\n",
            "{'ID': 46, 'is_correct': 'False'}\n",
            "{'ID': 47, 'is_correct': 'False'}\n",
            "{'ID': 48, 'is_correct': 'False'}\n",
            "{'ID': 49, 'is_correct': 'True'}\n",
            "{'ID': 50, 'is_correct': 'False'}\n",
            "{'ID': 51, 'is_correct': 'True'}\n",
            "{'ID': 52, 'is_correct': 'False'}\n",
            "{'ID': 53, 'is_correct': 'False'}\n",
            "{'ID': 54, 'is_correct': 'False'}\n",
            "{'ID': 55, 'is_correct': 'False'}\n",
            "{'ID': 56, 'is_correct': 'False'}\n",
            "{'ID': 57, 'is_correct': 'False'}\n",
            "{'ID': 58, 'is_correct': 'False'}\n",
            "{'ID': 59, 'is_correct': 'False'}\n",
            "{'ID': 60, 'is_correct': 'False'}\n",
            "{'ID': 61, 'is_correct': 'True'}\n",
            "{'ID': 62, 'is_correct': 'True'}\n",
            "{'ID': 63, 'is_correct': 'False'}\n",
            "{'ID': 64, 'is_correct': 'False'}\n",
            "{'ID': 65, 'is_correct': 'False'}\n",
            "{'ID': 66, 'is_correct': 'False'}\n",
            "{'ID': 67, 'is_correct': 'False'}\n",
            "{'ID': 68, 'is_correct': 'False'}\n",
            "{'ID': 69, 'is_correct': 'False'}\n",
            "{'ID': 70, 'is_correct': 'False'}\n",
            "{'ID': 71, 'is_correct': 'True'}\n",
            "{'ID': 72, 'is_correct': 'False'}\n",
            "{'ID': 73, 'is_correct': 'True'}\n",
            "{'ID': 74, 'is_correct': 'False'}\n",
            "{'ID': 75, 'is_correct': 'True'}\n",
            "{'ID': 76, 'is_correct': 'False'}\n",
            "{'ID': 77, 'is_correct': 'False'}\n",
            "{'ID': 78, 'is_correct': 'False'}\n",
            "{'ID': 79, 'is_correct': 'True'}\n",
            "{'ID': 80, 'is_correct': 'False'}\n",
            "{'ID': 81, 'is_correct': 'True'}\n",
            "{'ID': 82, 'is_correct': 'True'}\n",
            "{'ID': 83, 'is_correct': 'False'}\n",
            "{'ID': 84, 'is_correct': 'False'}\n",
            "{'ID': 85, 'is_correct': 'False'}\n",
            "{'ID': 86, 'is_correct': 'True'}\n",
            "{'ID': 87, 'is_correct': 'False'}\n",
            "{'ID': 88, 'is_correct': 'False'}\n",
            "{'ID': 89, 'is_correct': 'False'}\n",
            "{'ID': 90, 'is_correct': 'False'}\n",
            "{'ID': 91, 'is_correct': 'False'}\n",
            "{'ID': 92, 'is_correct': 'True'}\n",
            "{'ID': 93, 'is_correct': 'True'}\n",
            "{'ID': 94, 'is_correct': 'True'}\n",
            "{'ID': 95, 'is_correct': 'True'}\n",
            "{'ID': 96, 'is_correct': 'True'}\n",
            "{'ID': 97, 'is_correct': 'True'}\n",
            "{'ID': 98, 'is_correct': 'True'}\n",
            "{'ID': 99, 'is_correct': 'False'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# å¯ç”¨å¿«é€Ÿæ¨ç†æ¨¡å¼\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# è®¾ç½®æ‰¹é‡å¤§å°å’Œæ ·æœ¬æ€»æ•°\n",
        "batch_size = 32\n",
        "num_samples = len(test_dataset) # æ€»å…±å¤„ç†çš„æ ·æœ¬æ•°é‡\n",
        "\n",
        "results = []\n",
        "\n",
        "# é€æ‰¹å¤„ç†æ ·æœ¬\n",
        "for start_idx in tqdm(range(0, num_samples, batch_size), desc=\"Running inference in batches\"):\n",
        "    # è·å–å½“å‰æ‰¹æ¬¡çš„æ ·æœ¬\n",
        "    end_idx = min(start_idx + batch_size, num_samples)\n",
        "    batch_samples = test_dataset['question'][start_idx:end_idx]\n",
        "    batch_answers = test_dataset['answer'][start_idx:end_idx]\n",
        "\n",
        "    # æ„é€ è¾“å…¥æç¤º\n",
        "    input_prompts = [prompt.format(q, a, \"\") for q, a in zip(batch_samples, batch_answers)]\n",
        "\n",
        "    # ç¼–ç è¾“å…¥æç¤º\n",
        "    inputs = tokenizer(input_prompts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
        "    input_shape = inputs['input_ids'].shape[1]\n",
        "\n",
        "    # æ‰§è¡Œæ‰¹é‡ç”Ÿæˆ\n",
        "    outputs = model.generate(**inputs, max_new_tokens=5, use_cache=True, eos_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    # è§£ç å¹¶è§£ææ‰¹é‡ç”Ÿæˆçš„å“åº”\n",
        "    responses = tokenizer.batch_decode(outputs[:, input_shape:], skip_special_tokens=True)\n",
        "\n",
        "    for i, response in enumerate(responses):\n",
        "        # åˆ¤æ–­ç”Ÿæˆçš„å“åº”æ˜¯å¦åŒ…å«â€œTrueâ€æˆ–â€œFalseâ€\n",
        "        if 'true' in response.lower():\n",
        "            is_correct = 'True'\n",
        "        elif 'false' in response.lower():\n",
        "            is_correct = 'False'\n",
        "        else:\n",
        "            is_correct = \"æ— æ³•åˆ¤æ–­\"  # è‹¥ç”Ÿæˆçš„ç»“æœæ— æ³•åˆ¤æ–­\n",
        "\n",
        "        # ä¿å­˜ç»“æœ\n",
        "        results.append({'ID': start_idx + i, 'is_correct': is_correct})\n",
        "\n",
        "# æ‰“å°å‡ºæ‰€æœ‰ç»“æœ\n",
        "# for result in results:\n",
        "#     print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbX1510mgLD6",
        "outputId": "eebefe0c-863d-4a19-a5a3-956fa8ccfaa3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running inference in batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 313/313 [05:26<00:00,  1.04s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# å°†ç»“æœä¿å­˜ä¸º CSV æ–‡ä»¶\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv('test_set_predictions.csv', index=False)\n",
        "\n",
        "print(\"å…¨é‡æµ‹è¯•é›†æ¨ç†å®Œæˆï¼Œç»“æœå·²ä¿å­˜ä¸º 'test_set_predictions.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ne_0Gy-iY8a",
        "outputId": "2a08244b-c03f-4374-9df3-f8d3f0f84efe"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "å…¨é‡æµ‹è¯•é›†æ¨ç†å®Œæˆï¼Œç»“æœå·²ä¿å­˜ä¸º 'test_set_predictions.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NB-9qn19iHKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## saving model"
      ],
      "metadata": {
        "id": "pKt3vZoSeRvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")"
      ],
      "metadata": {
        "id": "VRiW2RQ0cWru",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b097b125-f571-4afa-d28b-555a1aa1daca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('lora_model/tokenizer_config.json',\n",
              " 'lora_model/special_tokens_map.json',\n",
              " 'lora_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n"
      ],
      "metadata": {
        "id": "paHfJLfVccmN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0c5befa-5368-4d8e-ce93-7540717d3636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2024.11.6: Fast Llama patching. Transformers = 4.46.2.\n",
            "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.5.1+cu124. CUDA = 8.0. CUDA Toolkit = 12.4.\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r lora_model.zip lora_model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ixdh_qP4jlb-",
        "outputId": "8140e2a5-3408-4c8b-d136-a936fbcb1681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: lora_model/ (stored 0%)\n",
            "  adding: lora_model/tokenizer.json (deflated 85%)\n",
            "  adding: lora_model/README.md (deflated 66%)\n",
            "  adding: lora_model/tokenizer_config.json (deflated 96%)\n",
            "  adding: lora_model/adapter_config.json (deflated 54%)\n",
            "  adding: lora_model/special_tokens_map.json (deflated 71%)\n",
            "  adding: lora_model/adapter_model.safetensors (deflated 7%)\n"
          ]
        }
      ]
    }
  ]
}